## ğŸ§  Introduction

**Vision-Language Models (VLMs)** have seen remarkable progress, especially with the rise of **mobile-friendly architectures** like [MobileVLM](https://github.com/Meituan-AutoML/MobileVLM). These lightweight models unlock a wide range of applications across edge devices and real-time scenarios. However, fine-tuning such models remains challenging due to the **high computational cost** and **modality complexity** (text + image).

To tackle this, **Low-Rank Adaptation (LoRA)** has been widely adopted as a parameter-efficient fine-tuning strategy. Yet, traditional LoRA with **fixed-rank** settings often lacks the flexibility and expressiveness needed for effectively training mobile VLMs.

### ğŸŒŠ Enter HyDRA

We present **HyDRA**, a novel **Hybrid Decomposed Rank Adaptation** framework designed for **efficient and adaptive fine-tuning of mobile VLMs**. HyDRA introduces:

- âœ… **Hierarchical Optimization**  
  - **Coarse-grained scheduling** assigns different ranks to different transformer layers.  
  - **Fine-grained tuning** further adjusts rank **within** each layer for better control and expressiveness.

- ğŸ” **Dynamic Rank Adjustment**  
  - A lightweight **performance prediction model** automatically learns and updates rank configurations **end-to-end during fine-tuning**.

### ğŸ“ˆ Key Benefits

- Up to **4.7% performance improvement** across multiple benchmarks, without increasing trainable parameter counts.
- Outperforms **fixed-rank LoRA** and even **full fine-tuning** in some tasks.
- Designed specifically for **mobile VLMs** with both image and text modalities.

## âš™ï¸ HyDRA Training Process

The training process of **HyDRA** also follows a **two-stage** design built on top of [MobileVLM](https://github.com/Meituan-AutoML/MobileVLM):

### ğŸ§© Stage I: Initialization Pretrain

â„ï¸ frozen vision encoder + ğŸ”¥ learnable LDP projector + â„ï¸ frozen LLM

- Training time: ~1â€“1.5 hours for it on 8Ã— A100 (80G)  


### ğŸ§  Stage II: Instruction Fine-tuning with Dynamic Rank Adaptation

â„ï¸ frozen vision encoder + ğŸ”¥ learnable LDP projector + ğŸ”¥ learnable LLM + âœ… hybrid-rank LoRA adapter + ğŸ” dynamic-rank scheduler

- This stage initializes coarse and fine-grained rank structures for different transformer layers.
- A lightweight performance model automatically adjusts LoRA ranks during fine-tuning.
- Training time: this training process takes around 2~3.5 hours for model on 8x A100 (80G) with a batch size of 128 and an average of approximately 46G/52G of GPU memory required.


> ğŸ“ **Note**: To train with fewer GPUs or lower memory, reduce `per_device_train_batch_size` and increase `gradient_accumulation_steps` accordingly to keep the **global batch size** constant:  
> `per_device_train_batch_size Ã— gradient_accumulation_steps Ã— num_gpus`

## 1ï¸âƒ£ Prepare Base Checkpoints

Download **MobileLLaMA** base models (1.7B / 2.7B) from Hugging Face:  
- [MobileLLaMA-1.7B](https://huggingface.co/mtgv/MobileLLaMA-1.4B-Chat)  
- [MobileLLaMA-2.7B](https://huggingface.co/mtgv/MobileLLaMA-2.7B-Chat)

Alternatively, you can skip this step â€” the model will be automatically downloaded by ğŸ¤— `transformers` during training.

---

## 2ï¸âƒ£ Prepare Data

Assume your project root is `/path/to/project/hydra`, organize your folders as:

cd /path/to/project/hydra
mkdir -p data/pretrain_data data/finetune_data data/benchmark_data

ğŸ“¦ Pretrain Data (for Stage I)

cd data/pretrain_data
Download LLaVA-558K dataset (provided by LLaVA team)
cd ${work_dir}/data/pretrain_data
Download the LLaVA-558K from here, which is provided by LLaVA team.
[LLaVA-558K ](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain)

ğŸ“š Instruction Tuning Data (for Stage II)

Download the instruction tuning annotations
Download the [LLaVA-Instruct-150K JSON file](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json) from Hugging Face.
Collect images from COCO, GQA, OCR-VQA, TextVQA, VisualGnome (Part1 & Part2)

ğŸ“šData Download Instructions

