## üß† Introduction

**Vision-Language Models (VLMs)** have seen remarkable progress, especially with the rise of **mobile-friendly architectures** like [MobileVLM](https://github.com/Meituan-AutoML/MobileVLM). These lightweight models unlock a wide range of applications across edge devices and real-time scenarios. However, fine-tuning such models remains challenging due to the **high computational cost** and **modality complexity** (text + image).

To tackle this, **Low-Rank Adaptation (LoRA)** has been widely adopted as a parameter-efficient fine-tuning strategy. Yet, traditional LoRA with **fixed-rank** settings often lacks the flexibility and expressiveness needed for effectively training mobile VLMs.

### üåä Enter HyDRA

We present **HyDRA**, a novel **Hybrid Decomposed Rank Adaptation** framework designed for **efficient and adaptive fine-tuning of mobile VLMs**. HyDRA introduces:

- ‚úÖ **Hierarchical Optimization**  
  - **Coarse-grained scheduling** assigns different ranks to different transformer layers.  
  - **Fine-grained tuning** further adjusts rank **within** each layer for better control and expressiveness.

- üîÅ **Dynamic Rank Adjustment**  
  - A lightweight **performance prediction model** automatically learns and updates rank configurations **end-to-end during fine-tuning**.

### üìà Key Benefits

- Up to **4.7% performance improvement** across multiple benchmarks, without increasing trainable parameter counts.
- Outperforms **fixed-rank LoRA** and even **full fine-tuning** in some tasks.
- Designed specifically for **mobile VLMs** with both image and text modalities.

## ‚öôÔ∏è HyDRA Training Process

The training process of **HyDRA** also follows a **two-stage** design built on top of [MobileVLM](https://github.com/Meituan-AutoML/MobileVLM):

### üß© Stage I: Initialization Pretrain

‚ùÑÔ∏è frozen vision encoder + üî• learnable LDP projector + ‚ùÑÔ∏è frozen LLM

- Training time: ~1‚Äì1.5 hours for it on 8√ó A100 (80G)  


### üß† Stage II: Instruction Fine-tuning with Dynamic Rank Adaptation

‚ùÑÔ∏è frozen vision encoder + üî• learnable LDP projector + üî• learnable LLM + ‚úÖ hybrid-rank LoRA adapter + üîÅ dynamic-rank scheduler

- This stage initializes coarse and fine-grained rank structures for different transformer layers.
- A lightweight performance model automatically adjusts LoRA ranks during fine-tuning.
- Training time: this training process takes around 2~3.5 hours for model on 8x A100 (80G) with a batch size of 128 and an average of approximately 46G/52G of GPU memory required.


> üìù **Note**: To train with fewer GPUs or lower memory, reduce `per_device_train_batch_size` and increase `gradient_accumulation_steps` accordingly to keep the **global batch size** constant:  
> `per_device_train_batch_size √ó gradient_accumulation_steps √ó num_gpus`

## 1Ô∏è‚É£ Prepare Base Checkpoints

Download **MobileLLaMA** base models (1.7B / 2.7B) from Hugging Face:  
- [MobileLLaMA-1.7B](https://huggingface.co/mtgv/MobileLLaMA-1.4B-Chat)  
- [MobileLLaMA-2.7B](https://huggingface.co/mtgv/MobileLLaMA-2.7B-Chat)

Alternatively, you can skip this step ‚Äî the model will be automatically downloaded by ü§ó `transformers` during training.

---

## 2Ô∏è‚É£ Prepare Data

Assume your project root is `/path/to/project/hydra`, organize your folders as:

cd /path/to/project/hydra
mkdir -p data/pretrain_data data/finetune_data data/benchmark_data

üì¶ Pretrain Data (for Stage I)

cd data/pretrain_data
Download LLaVA-558K dataset (provided by LLaVA team)
cd ${work_dir}/data/pretrain_data
Download the LLaVA-558K from here, which is provided by LLaVA team.
[LLaVA-558K ](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain)

üìö Instruction Tuning Data (for Stage II)

Download the instruction tuning annotations
Download the [LLaVA-Instruct-150K JSON file](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json) from Hugging Face.
Collect images from COCO, GQA, OCR-VQA, TextVQA, VisualGnome (Part1 & Part2)

üìöData Download Instructions
download some useful data/scripts pre-collected.
unzip benchmark_data.zip && cd benchmark_data
bmk_dir=${work_dir}/data/benchmark_data
gqa
download its image data following the official instructions here
cd ${bmk_dir}/gqa && ln -s /path/to/gqa/images images
mme
download the data following the official instructions here.
cd ${bmk_dir}/mme && ln -s /path/to/MME/MME_Benchmark_release_version images
pope
download coco from POPE following the official instructions here.
cd ${bmk_dir}/pope && ln -s /path/to/pope/coco coco && ln -s /path/to/coco/val2014 val2014
sqa
download images from the data/scienceqa folder of the ScienceQA repo.
cd ${bmk_dir}/sqa && ln -s /path/to/sqa/images images
textvqa
download images following the instructions here.
cd ${bmk_dir}/textvqa && ln -s /path/to/textvqa/train_images train_images
mmbench
no action is needed.
